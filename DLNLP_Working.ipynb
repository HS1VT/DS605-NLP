{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR3XpbZ5402V",
        "outputId": "53384bed-16a7-4c32-b3dd-c49b9fd1a36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries\n",
        "!pip install fasttext umap-learn pandas\n",
        "\n",
        "# Import necessary libraries\n",
        "import fasttext\n",
        "import umap\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PAPER REPLICATION"
      ],
      "metadata": {
        "id": "yAgK4phbeMX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import procrustes\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "-h3ApO7HeRn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_set(dataset_path):\n",
        "    foo = open(dataset_path, 'r')\n",
        "    foow = open(dataset_path[:-4]+'.csv', 'a')\n",
        "    content = foo.readlines()\n",
        "\n",
        "    foow.write('Word,English\\n')\n",
        "    for i in content:\n",
        "      create = i.split()\n",
        "      if len(create) >= 2:\n",
        "        a = create[0]\n",
        "        b = create[1]\n",
        "        foow.write(a+','+b+'\\n')\n",
        "    foo.close()\n",
        "    foow.close()"
      ],
      "metadata": {
        "id": "natGJKzogt0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_training(dataset_path, eng_mod_name, new_mod_name, lang):\n",
        "    # LOADING DATASET\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    column_names = list(df.columns.values)\n",
        "    # Display the first few rows of the dataset\n",
        "    df.head()\n",
        "\n",
        "    # MODEL TRAINING\n",
        "    # Combine English and New Language words into a single list\n",
        "    all_words = df[column_names[1]].tolist() + df[column_names[0]].tolist()\n",
        "\n",
        "    # Save the combined words to a text file for training FastText\n",
        "    with open('temp.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(str(all_words)))\n",
        "\n",
        "    # Train FastText model for English\n",
        "    #english_model_path = 'english_fasttext_model.bin'\n",
        "    english_model_path = eng_mod_name\n",
        "    english_model = fasttext.train_unsupervised('temp.txt', model='skipgram')\n",
        "    english_model.save_model(english_model_path)\n",
        "\n",
        "    # Train FastText model for the New Language\n",
        "    #new_language_model_path = 'new_language_fasttext_model.bin'\n",
        "    new_language_model_path = new_mod_name\n",
        "    new_language_model = fasttext.train_unsupervised('temp.txt', model='skipgram')\n",
        "    new_language_model.save_model(new_language_model_path)\n",
        "\n",
        "    #GETTING VECTORS\n",
        "    # English\n",
        "    english_word_vectors = {word: english_model.get_word_vector(word) for word in df[column_names[1]].tolist()}\n",
        "    # New Language word vectors\n",
        "    new_language_word_vectors = {word: new_language_model.get_word_vector(word) for word in df[column_names[0]].tolist()}\n",
        "\n",
        "\n",
        "    #PROCRUSTES ALIGNMENT\n",
        "\n",
        "    eng_vec = []\n",
        "    lang_vec = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        eng_vec.append(english_model.get_word_vector(df[column_names[1]].tolist()[i]))\n",
        "        lang_vec.append(new_language_model.get_word_vector(df[column_names[0]].tolist()[i]))\n",
        "\n",
        "    mtx1, mtx2, disparity = procrustes(eng_vec, lang_vec)\n",
        "\n",
        "    score1 = metrics.mean_squared_error(mtx1, mtx2)\n",
        "\n",
        "    return english_word_vectors, new_language_word_vectors, df"
      ],
      "metadata": {
        "id": "ObGvifkZc-NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#UMAP ALIGNMENT\n",
        "\n",
        "def umap_alignment(eng_vec, new_vec, lang, df):\n",
        "\n",
        "    # Concatenate English and New Language vectors for alignment\n",
        "    # all_vectors = list(english_word_vectors.values()) + list(new_language_word_vectors.values())\n",
        "    all_vectors = list(eng_vec.values()) + list(new_vec.values())\n",
        "\n",
        "    # Use UMAP to align vectors\n",
        "    mapper = umap.UMAP()\n",
        "    aligned_vectors = mapper.fit_transform(all_vectors)\n",
        "\n",
        "    # Split the aligned vectors back into English and New Language\n",
        "    aligned_english_vectors = aligned_vectors[:len(eng_vec)]\n",
        "    aligned_new_language_vectors = aligned_vectors[len(eng_vec):]\n",
        "\n",
        "    # Save or use the aligned vectors as needed\n",
        "    aligned_english_vectors_df = pd.DataFrame(aligned_english_vectors, columns=['Aligned_English_1', 'Aligned_English_2'])\n",
        "    aligned_new_language_vectors_df = pd.DataFrame(aligned_new_language_vectors, columns=['Aligned_New_Language_1', 'Aligned_New_Language_2'])\n",
        "\n",
        "    aligned_df = pd.concat([df, aligned_english_vectors_df, aligned_new_language_vectors_df], axis=1)\n",
        "\n",
        "    # Save the aligned vectors to a new CSV file\n",
        "    aligned_df.dropna(axis = 0)\n",
        "    aligned_df.to_csv('aligned_vectors_'+lang+'.csv', index=False)\n",
        "\n",
        "    # GETTING SCORES\n",
        "    foo = open('aligned_vectors_'+lang+'.csv', 'r')\n",
        "    content = foo.readlines()\n",
        "    foo.close()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for i in range(1, len(content)):\n",
        "        bw, w, ae1, ae2, ab1, ab2 = content[i].split(',')\n",
        "        if ae1 != \"\" and ae2!= \"\" and ab1 != \"\" and ab2!= \"\":\n",
        "            y_true.append([float(ae1), float(ae2)])\n",
        "            y_pred.append([float(ab1), float(ab2)])\n",
        "\n",
        "    score = metrics.mean_squared_error(y_true, y_pred)"
      ],
      "metadata": {
        "id": "PnjEiELYeRn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COMPILED\n",
        "\n",
        "files = ['af-en.txt', 'als-en.txt', 'az-en.txt', 'jv-en.txt', 'kn-en.txt', 'sco-en.txt', 'tg-en.txt']\n",
        "languages = ['af', 'als', 'az', 'jv', 'kn', 'sco', 'sh', 'tg']\n",
        "\n",
        "def combine_run(file_path, lang):\n",
        "    create_set(file_path)\n",
        "    eng_vec, lang_vec, lang_df = model_training(file_path[:-4]+'.csv', 'en-'+lang+'-model.bin', lang+'-eng-model.bin', lang)\n",
        "    umap_alignment(eng_vec, lang_vec, lang, lang_df)\n",
        "\n",
        "for i in range(len(files)):\n",
        "    combine_run(files[i], languages[i])"
      ],
      "metadata": {
        "id": "IuFVeexjn_ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COMPILED\n",
        "\n",
        "files = ['bodo-en.txt']\n",
        "languages = ['bodo']\n",
        "\n",
        "def combine_run(file_path, lang):\n",
        "    create_set(file_path)\n",
        "    eng_vec, lang_vec, lang_df = model_training(file_path[:-4]+'.csv', 'en-'+lang+'-model.bin', lang+'-eng-model.bin', lang)\n",
        "    umap_alignment(eng_vec, lang_vec, lang, lang_df)\n",
        "\n",
        "for i in range(len(files)):\n",
        "    combine_run(files[i], languages[i])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "moCN-_bk_SXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}